wandb: Currently logged in as: jongsong. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.14.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.10
wandb: Run data is saved locally in /home/jongsong/BLINK/wandb/run-20230321_143306-ozox6hze
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-terrain-3202
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jongsong/BLINK-blink_crossencoder
wandb: üöÄ View run at https://wandb.ai/jongsong/BLINK-blink_crossencoder/runs/ozox6hze
Epoch:   0%|          | 0/10 [00:00<?, ?it/s]
Batch:   0%|          | 0/3015 [00:00<?, ?it/s][ABatch:   0%|          | 0/3015 [00:05<?, ?it/s]
Epoch:   0%|          | 0/10 [00:05<?, ?it/s]
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:    acc/train_bi-encoder_accuracy ‚ñÅ
wandb:      acc/val_bi-encoder_accuracy ‚ñÅ
wandb:         mrr/train_bi-encoder_mrr ‚ñÅ
wandb:           mrr/val_bi-encoder_mrr ‚ñÅ
wandb: recall/train_bi-encoder_recall@4 ‚ñÅ
wandb:   recall/val_bi-encoder_recall@4 ‚ñÅ
wandb: 
wandb: Run summary:
wandb:    acc/train_bi-encoder_accuracy 0.56803
wandb:      acc/val_bi-encoder_accuracy 0.46344
wandb:         mrr/train_bi-encoder_mrr 0.69881
wandb:           mrr/val_bi-encoder_mrr 0.61246
wandb: recall/train_bi-encoder_recall@4 0.83689
wandb:   recall/val_bi-encoder_recall@4 0.76217
wandb: 
wandb: üöÄ View run dandy-terrain-3202 at: https://wandb.ai/jongsong/BLINK-blink_crossencoder/runs/ozox6hze
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230321_143306-ozox6hze/logs
Traceback (most recent call last):
  File "blink/crossencoder/train_cross.py", line 992, in <module>
    main(params)
  File "blink/crossencoder/train_cross.py", line 567, in main
    loss, _ = reranker(context_input, label_input, context_length)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "./blink/crossencoder/crossencoder.py", line 156, in forward
    scores = self.score_candidate(input_idx, context_len)
  File "./blink/crossencoder/crossencoder.py", line 223, in score_candidate
    embedding_ctxt = self.model(token_idx_ctxt, segment_idx_ctxt, mask_ctxt,token_idx_cands, segment_idx_cands, mask_cands)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 171, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 181, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 89, in parallel_apply
    output.reraise()
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
torch.cuda.OutOfMemoryError: Caught OutOfMemoryError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 64, in _worker
    output = module(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "./blink/crossencoder/crossencoder.py", line 196, in forward
    embedding_ctxt = self.encoder(token_idx_ctxt, segment_idx_ctxt, mask_ctxt)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "./blink/common/ranker_base_cross.py", line 29, in forward
    token_ids, segment_ids, attention_mask
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 711, in forward
    head_mask=head_mask)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 432, in forward
    layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 412, in forward
    intermediate_output = self.intermediate(attention_output)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/pytorch_transformers/modeling_bert.py", line 383, in forward
    hidden_states = self.dense(hidden_states)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/jongsong/anaconda3/envs/blink/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 23.70 GiB total capacity; 21.93 GiB already allocated; 264.31 MiB free; 22.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

slurmstepd: error: *** JOB 299937 ON c07 CANCELLED AT 2023-03-21T15:25:38 ***
